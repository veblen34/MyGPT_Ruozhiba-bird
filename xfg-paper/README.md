# 论文文本分类任务



### 采用ChatGLM3-6B与LoRA微调方法

- 本次微调在L20-48GB上进行微调
- 注意peft版本应保持在0.4.0以下，0.7.0以上的版本对浮点数的支持有所变化



### Reference

清华大学and智源AI的开源预训练模型

git项目地址：

[https://github.com/THUDM/ChatGLM3](https:/github.com/THUDM/ChatGLM3)[THUDM/ChatGLM3: ChatGLM3 series: Open Bilingual Chat LLMs | 开源双语对话语言模型 (github.com)](https://github.com/THUDM/ChatGLM3)

